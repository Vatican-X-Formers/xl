# Base
text8: &text8
   dataset: text8
   data: data/text8/

train: &train
   <<: *text8
   cuda: true
   n_layer: 0
   d_model: 512
   n_head: 8
   d_head: 64
   d_inner: 2048
   dropout: 0.1
   dropatt: 0.0
   optim: adam
   lr: 0.00025
   warmup_step: 0
   max_step: 200000
   tgt_len: 2048
   mem_len: 0
   eval_tgt_lengths: [512]
   eval_total_lengths: [2048]
   batch_size: 8
   multi_gpu: ddp
   log_interval: 100
   eval_interval: 10000
   text_generation_interval: 25000
   batch_chunk: 1
   pre_lnorm: false
   funnel_config: "[2, (8, 1), 2]"
   funnel_resample: custom
   activation_function: relu
   boundaries_type: 'tokenizer'
   tokenizer_type: 'unigram'
   tokenizer_vocab_size: 5000
   tokenizer_dropout: 0.0
   tokenizer_algorithm: 'approach1'

default:
   train:
      <<: *train
