wiki40b_fi: &wiki40b_fi
   dataset: wiki40b_fi
   data: data/wiki40b/fi/

model: &model
   d_model: 512
   n_head: 8
   d_head: 64
   d_inner: 2048
   dropout: 0.1
   dropatt: 0.0
   pre_lnorm: false
   funnel_config: "[2, (8,), 2]"
   downsample_mode: 'average'
   upsample_mode: 'average'
   activation_function: relu
   gather_stats: ['shortened_length']
   mask_mode: 'boundary_ends_group'

# The main set of argument for boundaries is here
boundaries: &boundaries
   boundaries_type: 'noboundaries' # Here I define boundaries created in data loader
   boundary_ids: '["_"]' # Used if type == ids
   bp_target: ['entropy'] # Here I define boundaries that can be extracted from LM objective
   fixed_sf: 0 # Used if type == constant
   n_iters: 0 # Used if bp_target is not None
   spikes_upper_perc: 100 # Used for spikes idea
   spikes_lower_perc: 0 # Used for spikes idea
   value_perc: 70 # Used for calculating boundaries from data
   group_threshold: 0.0
   spikes_step: 100

# If I want to add parameters on top of Transformer layers
# I do this using this group of params that define BP
bp: &bp
   bp_mode: 'default'
   bp_capacity: 'nonlinear'
   bp_switch_step: 0 # This means I don't use teacher forcing

# This set of params is used if and only if boundaries_type = tokenizer
tokenizer: &tokenizer
   tokenizer_type: 'unigram'
   tokenizer_vocab_size: 5000

eval: &eval
   eval_interval: 5000
   eval_max_steps: 200
   eval_tgt_lengths: [512]
   eval_total_lengths: [2048]
   text_generation_interval: 25000

optim: &optim
   optim: adam
   scheduler: cosine
   lr: 0.00025
   warmup_step: 1000
   clip: 0.25
   weight_decay: 0
   adam_b1: 0.9
   adam_b2: 0.999
   adam_eps: 1e-8

train: &train
   affinity: disabled
   multi_gpu: ddp
   cuda: true
   max_step: 200000
   tgt_len: 2048
   batch_size: 8
   batch_chunk: 2
   log_interval: 100

default:
   train:
      <<: *wiki40b_fi
      <<: *model
      <<: *bp
      <<: *tokenizer
      <<: *boundaries
      <<: *eval
      <<: *optim
      <<: *train
